from datetime import datetime
import time
from io import BytesIO
from pathlib import Path
from fastapi.responses import FileResponse, StreamingResponse
import torch
import numpy as np
from diffusers import WanPipeline, AutoencoderKLWan, WanTransformer3DModel, UniPCMultistepScheduler
from diffusers.utils import export_to_video, load_image
import modal

cuda_version = "12.4.0"  # should be no greater than host CUDA version
flavor = "devel"  # includes full CUDA toolkit
operating_sys = "ubuntu22.04"
tag = f"{cuda_version}-{flavor}-{operating_sys}"

VOLUME_NAME = "wan2-outputs"
OUTPUTS_PATH = Path("/outputs")  # remote path for saving video outputs
outputs = modal.Volume.from_name(VOLUME_NAME, create_if_missing=True)


cuda_dev_image = modal.Image.from_registry(
    f"nvidia/cuda:{tag}", add_python="3.11"
).entrypoint([])

diffusers_commit_sha = "dbe413668dbf9d527944b51ef5f99a60b36a90be"

wan2_image = (
    cuda_dev_image.apt_install(
        "git",
        "libglib2.0-0",
        "libsm6",
        "libxrender1",
        "libxext6",
        "ffmpeg",
        "libgl1",
    )
    .pip_install(
        "invisible_watermark==0.2.0",
        "transformers",
        "huggingface_hub[hf_transfer]",
        "accelerate==0.33.0",
        "safetensors==0.4.4",
        "sentencepiece==0.2.0",
        "torch==2.5.0",
        f"git+https://github.com/huggingface/diffusers.git",
        "numpy<2",
        "hf_transfer",
        "ftfy",
        "imageio",
        "imageio-ffmpeg",
        "fastapi[standard]"
    )
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1", "HF_HUB_CACHE": "/cache"})
)

wan2_image = wan2_image.env(
    {
        "TORCHINDUCTOR_CACHE_DIR": "/root/.inductor-cache",
        "TORCHINDUCTOR_FX_GRAPH_CACHE": "1",
    }
)

app = modal.App("example-wan2", image=wan2_image)

with wan2_image.imports():
    import torch
    from diffusers import DiffusionPipeline


MINUTES = 60  # seconds
VARIANT = "schnell"  # or "dev"
NUM_INFERENCE_STEPS = 4  # use ~50 for [dev], smaller for [schnell]


@app.cls(
    gpu="H200",  # fast GPU with strong software support
    scaledown_window= 2 * MINUTES,
    timeout=5 * MINUTES,  # leave plenty of time for compilation
    volumes={  # add Volumes to store serializable compilation artifacts, see section on torch.compile below
        "/cache": modal.Volume.from_name("hf-hub-cache", create_if_missing=True),
        "/root/.nv": modal.Volume.from_name("nv-cache", create_if_missing=True),
        "/root/.triton": modal.Volume.from_name("triton-cache", create_if_missing=True),
        "/root/.inductor-cache": modal.Volume.from_name(
            "inductor-cache", create_if_missing=True
        ),
        OUTPUTS_PATH: outputs,  # videos will be saved to a distributed volume
    },
    secrets=[modal.Secret.from_name("huggingface-secret")],
)
class Model:
    compile: bool = (  # see section on torch.compile below for details
        modal.parameter(default=False)
    )

    @modal.enter()
    def enter(self):
        dtype = torch.bfloat16
        device = "cuda"
        model_id = "Wan-AI/Wan2.2-T2V-A14B-Diffusers"
        vae = AutoencoderKLWan.from_pretrained(model_id, subfolder="vae", torch_dtype=torch.float32)
        self.pipe = WanPipeline.from_pretrained(model_id, vae=vae, torch_dtype=dtype)
        self.pipe.to(device)



    @modal.method()
    def inference(self, prompt: str) -> bytes:
        print(f"ðŸŽ¨ generating video with prompt: {prompt}")

        height = 704
        width = 1280
        num_frames = 121
        num_inference_steps = 50
        guidance_scale = 5.0
        negative_prompt = "lowres, bad anatomy, error body, error arm, error hand, error fingers, error legs, error feet, missing fingers"
        output = self.pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            height=height,
            width=width,
            num_frames=num_frames,
            guidance_scale=guidance_scale,
            num_inference_steps=num_inference_steps,
        ).frames[0]
        print("ðŸŽ¨ generation complete, exporting video...")
        filename = f"output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4"
        export_to_video(output, f"/outputs/{filename}", fps=24)
        return filename



def optimize(pipe, compile=True):
    # fuse QKV projections in Transformer and VAE
    pipe.transformer.fuse_qkv_projections()
    pipe.vae.fuse_qkv_projections()

    # switch memory layout to Torch's preferred, channels_last
    pipe.transformer.to(memory_format=torch.channels_last)
    pipe.vae.to(memory_format=torch.channels_last)

    if not compile:
        return pipe

    # set torch compile flags
    config = torch._inductor.config
    config.disable_progress = False  # show progress bar
    config.conv_1x1_as_mm = True  # treat 1x1 convolutions as matrix muls
    # adjust autotuning algorithm
    config.coordinate_descent_tuning = True
    config.coordinate_descent_check_all_directions = True
    config.epilogue_fusion = False  # do not fuse pointwise ops into matmuls

    # tag the compute-intensive modules, the Transformer and VAE decoder, for compilation
    pipe.transformer = torch.compile(
        pipe.transformer, mode="max-autotune", fullgraph=True
    )
    pipe.vae.decode = torch.compile(
        pipe.vae.decode, mode="max-autotune", fullgraph=True
    )

    # trigger torch compilation
    print("ðŸ”¦ running torch compilation (may take up to 20 minutes)...")

    pipe(
        "dummy prompt to trigger torch compilation",
        output_type="pil",
        num_inference_steps=NUM_INFERENCE_STEPS,  # use ~50 for [dev], smaller for [schnell]
    ).images[0]

    print("ðŸ”¦ finished torch compilation")

    return pipe
model = Model()
@app.local_entrypoint()
def main(
    prompt: str = "A black scottish terrier (scottie dog) wandering around a steampunk city, hyperrealistic, 4k",
    compile: bool = False,
):
    pass
    # video_filename = model.inference.remote(prompt)

    # output_path = Path("/tmp") / "wan2" / "output.mp4"
    # output_path.parent.mkdir(exist_ok=True, parents=True)
    # print(f"ðŸŽ¨ saving output to {output_path}")
    # out_bytes = outputs.read_file(video_filename)
    # with open(output_path, "wb") as f:
    #     for chunk in out_bytes:
    #         f.write(chunk)
    # print(f"ðŸŽ¨ saved output to {output_path}")

def read_file_chunks(filename: str):
    vol = modal.Volume.from_name("wan2-outputs")
    for chunk in vol.read_file(filename):
        yield chunk

@app.function()
@modal.fastapi_endpoint()
def generate_video_http(prompt: str) -> str:
    """Generate a video from a text prompt using WAN2 model and return the filename."""
    filename = model.inference.remote(prompt)
    # print(filename, str(filename))
    # filename = "output_20251018_023241.mp4"
    time.sleep(10)  # wait for the file to be ready
    return StreamingResponse(
        read_file_chunks(filename),
        media_type="video/mp4",
        headers={"Content-Disposition": f"attachment; filename=\"{filename}\""}
    )